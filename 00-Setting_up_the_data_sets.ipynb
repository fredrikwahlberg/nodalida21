{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ad9RMOuy0d2l"
   },
   "source": [
    "# Download, pre-process and run feature extraction on the data sets\n",
    "\n",
    "Joakim: Word n-gram for facsimile data. Divergences: Jensen-Shannon, KL, Jeffreys.\n",
    "\n",
    "Bea: Add menota data http://clarino.uib.no/menota/catalogue, mostly norwegian and islandic, HistCorp https://cl.lingfil.uu.se/histcorp/index.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2QX4-R9rxGFb"
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import re\n",
    "from multiprocessing import Pool\n",
    "\n",
    "# Web\n",
    "from urllib.request import urlopen, urlretrieve\n",
    "\n",
    "# File system\n",
    "import os\n",
    "import os.path\n",
    "\n",
    "# For downloading and using external libraries\n",
    "import sys\n",
    "sys.path.append('external')\n",
    "assert os.path.exists('external')\n",
    "\n",
    "# Data location\n",
    "assert os.path.exists(\"data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "\n",
    "Arabic and roman numerals are blanked out from all documents. Instead of making a 'smart' regex for all numerals, we generate all numbers (arabic and roman) that we want to remove. This long regex of special cases is compiled to make it faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from roman import toRoman\n",
    "\n",
    "tokens = ['NUMBER\\_TOKEN']\n",
    "for n in range(1100, 2020+1):                                      # The range of years to remove\n",
    "    tokens.append(\"%i\" % n)                                        # Adds the arabic number\n",
    "    r = toRoman(n)                                                 # Adds the roman number\n",
    "    tokens.append(\"\\.*\".join(list(r)))                             # Adds zero or more dots between roman 'digits'\n",
    "lengths = np.asarray([len(token) for token in tokens])\n",
    "tokens = list(np.asarray(tokens)[np.argsort(lengths)[::-1]])\n",
    "numerals_regexp = \"(\" + \"|\".join(tokens) + \")\"                     # Converts the list of special cases into a regex string\n",
    "numerals_regexp = r\"(?<=\\b)\"+numerals_regexp+r\"(?=\\b)\"             # Adds word limits\n",
    "#print(numerals_regexp)\n",
    "numerals_regexp = re.compile(numerals_regexp, flags=re.IGNORECASE) # Compiles the regex to make the list of specal cases faster\n",
    "\n",
    "# Tests\n",
    "m = numerals_regexp\n",
    "#exp = re.compile(r'[MDCLX]*[MDCLXVI]{1,3}')\n",
    "#exp = re.compile(\"M{0,4}\\.*(CM|CD|D?C{0,3})\\.*(XC|XL|L?X{0,3})\\.*(IX|IV|V?I){1,3}\")\n",
    "#print(exp.findall(\"I'm a green dream\"))\n",
    "#print(exp.findall(\"M.DCCCVI\"))\n",
    "assert len(m.findall(\"i\")) == 0\n",
    "assert len(m.findall(\"iv\")) == 0\n",
    "assert len(m.findall(\"i.v\")) == 0\n",
    "assert len(m.findall(\"Iv\")) == 0\n",
    "assert len(m.findall(\"I'm a green dream\")) == 0\n",
    "assert len(m.findall(\"MDIII\")) > 0\n",
    "assert len(m.findall(\"MDIiI\")) > 0\n",
    "assert len(m.findall(\"MD.IiI\")) > 0\n",
    "assert len(m.findall(\"mdiii\")) > 0\n",
    "assert len(m.findall(\"M.DCCCVI\")) > 0\n",
    "assert '2000' in m.findall(\"2000 lkfdjöd fösddlfk lorem ipsum1829 1943\")\n",
    "assert '1943' in m.findall(\"2000 lkfdjöd fösddlfk lorem ipsum1829 1943\")\n",
    "assert '1829' not in m.findall(\"2000 lkfdjöd fösddlfk lorem ipsum1829 1943\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "import html\n",
    "import regex\n",
    "\n",
    "def replace_html_entities(text):\n",
    "    return html.unescape(text)\n",
    "\n",
    "def remove_repeated_space(text):\n",
    "    text = regex.sub(r\" +\", \" \", text)\n",
    "    return text\n",
    "\n",
    "def remove_html_tags(text):\n",
    "    text = regex.sub(r\"<.*?>\", \"\", text)\n",
    "    return text\n",
    "\n",
    "def remove_footnotes(text):\n",
    "    text = regex.sub(r\"\\(\\d+\\)\", \"\", text)\n",
    "    return text\n",
    "\n",
    "def remove_parentheses(text):\n",
    "    text = regex.sub(r\"\\p{Ps}|\\p{Pe}\", \"\", text)\n",
    "    return text\n",
    "\n",
    "def split_regroup(match, group=1, join_by=\" \", prefix=\" \", suffix=\"\"):\n",
    "    return prefix+join_by.join(list(match.group(group)))+suffix\n",
    "\n",
    "def split_punctuation_from_word(text):\n",
    "    \"\"\" Split punctuation from beginning and end of words\n",
    "    \"\"\"\n",
    "    text = regex.sub(r\"(?<=\\s|^)(\\p{p}+)\\b\", r\"\\1 \", text)\n",
    "    text = regex.sub(r\"\\b(\\p{p}+)(?=\\s|$)\", r\" \\1\", text)\n",
    "    return text\n",
    "\n",
    "def split_punctuation(text):\n",
    "    \"\"\" Split sequence of punctuation\n",
    "    \"\"\"\n",
    "    text = regex.sub(r\"(?<=\\s|^)(\\p{p}{2,})(?=\\s|$)\", partial(split_regroup, prefix=\"\", suffix=\"\"), text)\n",
    "    return text\n",
    "\n",
    "def substitute_number(text, replace=\"<NUM>\"):#, predefined=[r\"NUMBER\\_TOKEN\"]):\n",
    "    text = numerals_regexp.sub(replace, text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProcessPipe(object):\n",
    "    def __init__(self, *processes):\n",
    "        self.processes = processes\n",
    "    def transform(self, input):\n",
    "        for process in self.processes:\n",
    "            input = process(input)\n",
    "        return input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess = [replace_html_entities, remove_html_tags, remove_footnotes, remove_parentheses, split_punctuation_from_word, split_punctuation]\n",
    "postproccess = [remove_repeated_space, lambda x: x.strip()]\n",
    "\n",
    "textprocess = ProcessPipe(*preprocess, substitute_number, *postproccess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tdBCdN-mdrDv"
   },
   "source": [
    "# Process data sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ekmL8uvWUSQ4"
   },
   "source": [
    "## SemEval 2015\n",
    "\n",
    "[Data description](http://alt.qcri.org/semeval2015/task7/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 153
    },
    "colab_type": "code",
    "id": "ZLRbNL0gAjBQ",
    "outputId": "6113d923-96e1-40f1-a75a-c9284d3c0586"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  data/original/SemEval2015_1.zip\n",
      "   creating: data/original/training08/\n",
      "  inflating: data/original/training08/training08T1.txt  \n",
      "   creating: data/original/__MACOSX/\n",
      "   creating: data/original/__MACOSX/training08/\n",
      "  inflating: data/original/__MACOSX/training08/._training08T1.txt  \n",
      "  inflating: data/original/training08/training08T2.txt  \n",
      "Archive:  data/original/SemEval2015_2.zip\n",
      "   creating: data/original/moreTraining/\n",
      "  inflating: data/original/moreTraining/training12T2.txt  \n",
      "  inflating: data/original/moreTraining/training12T1.txt  \n"
     ]
    }
   ],
   "source": [
    "fn = os.path.join('data', 'original', 'SemEval2015_1.zip')\n",
    "if not os.path.exists(fn):\n",
    "    data_url = \"\"\"http://alt.qcri.org/semeval2015/task7/data/uploads/training08.zip\"\"\"\n",
    "    urlretrieve(data_url, fn)\n",
    "\n",
    "fn = os.path.join('data', 'original', 'SemEval2015_2.zip')\n",
    "if not os.path.exists(fn):\n",
    "    data_url = \"\"\"http://alt.qcri.org/semeval2015/task7/data/uploads/moretraining.zip\"\"\"\n",
    "    urlretrieve(data_url, fn)\n",
    "\n",
    "!unzip -o data/original/SemEval2015_1.zip -d data/original\n",
    "!unzip -o data/original/SemEval2015_2.zip -d data/original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "4Lqjuq-1H1f1",
    "outputId": "b366bdd6-801b-4dcd-e11f-b52fd99468c8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wc: illegal option -- -\n",
      "usage: wc [-clmw] [file ...]\n",
      "wc: illegal option -- -\n",
      "usage: wc [-clmw] [file ...]\n",
      "wc: illegal option -- -\n",
      "usage: wc [-clmw] [file ...]\n",
      "wc: illegal option -- -\n",
      "usage: wc [-clmw] [file ...]\n"
     ]
    }
   ],
   "source": [
    "!grep '<text ' data/original/training08/training08T1.txt | wc --lines\n",
    "!grep '<text ' data/original/training08/training08T2.txt | wc --lines\n",
    "!grep '<text ' data/original/moretraining/training12T1.txt | wc --lines\n",
    "!grep '<text ' data/original/moretraining/training12T2.txt | wc --lines\n",
    "# !head -n 10 training08/training08T2.txt\n",
    "# !grep -v '<' training08/training08T2.txt | head -n 10\n",
    "# !grep '<text ' training08/training08T2.txt | head -n 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "oOaJw3axYrIg",
    "outputId": "2d721803-8319-4c5d-84f9-60ac71819c2d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1673/1673 [00:00<00:00, 4153.39it/s]\n",
      "  2%|▏         | 362/22008 [00:00<00:06, 3582.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Errors: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22008/22008 [00:05<00:00, 4229.88it/s]\n",
      "100%|██████████| 469/469 [00:00<00:00, 3241.57it/s]\n",
      "  0%|          | 0/7168 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Errors: 0\n",
      "Errors: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7168/7168 [00:02<00:00, 3500.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Errors: 111\n",
      "Found 4424 data items\n"
     ]
    }
   ],
   "source": [
    "def parse(textlines, subtask):\n",
    "  errors = 0\n",
    "  \"\"\"Parse the data from one semeval file. Text should be split into lines.\"\"\"\n",
    "  import re\n",
    "  import numpy as np\n",
    "  regex = r'(?P<tag>yes|id|no)=\\\"(?P<data>[a-zA-Z0-9\\-]+)\\\"'\n",
    "  data = list()\n",
    "  for line in tqdm(textlines):\n",
    "    if line[:6].lower() == '<text ':\n",
    "      # s = line[:6][:-1]\n",
    "      y = [-np.inf, np.inf]\n",
    "      e = list(re.finditer(regex, line))[0]\n",
    "      item = {e.groupdict()['tag']: e.groupdict()['data'], 'subtask': subtask}\n",
    "    elif line[:6].lower() == '<textf' or line[:6].lower() == '<textm' or line[:6].lower() == '<textc':\n",
    "      # Dating data\n",
    "      for match in re.finditer(regex, line):\n",
    "        if match.groupdict()['tag'] == 'yes':\n",
    "          s = match.groupdict()['data'].split(\"-\")\n",
    "          try:\n",
    "              y[0] = max(y[0], int(s[0]))\n",
    "              y[1] = min(y[1], int(s[1]))\n",
    "          except ValueError:\n",
    "              y[0] = None\n",
    "              y[1] = None\n",
    "              errors+=1\n",
    "              \n",
    "    elif len(line) > 10 and line[0] != '<':\n",
    "      item['text'] = textprocess.transform(line)\n",
    "    elif line[:6].lower() == '</text':\n",
    "      item['date'] = tuple(y)\n",
    "      if 'text' in item.keys():\n",
    "        data.append(item)\n",
    "\n",
    "  return data, errors\n",
    "\n",
    "with open('data/original/training08/training08T1.txt', 'r') as file:\n",
    "  textlines = [tl.strip() for tl in file.readlines()]\n",
    "  data, e = parse(textlines, subtask = 1)\n",
    "  data = [x for x in data if x[\"date\"][0]!=None]\n",
    "  print(\"Errors:\", e)\n",
    "with open('data/original/training08/training08T2.txt', 'r') as file:\n",
    "  textlines = [tl.strip() for tl in file.readlines()]\n",
    "  d, e = parse(textlines, subtask = 2)\n",
    "  data.extend(d)\n",
    "  print(\"Errors:\", e)    \n",
    "\n",
    "with open('data/original/moretraining/training12T1.txt', 'r') as file:\n",
    "  textlines = [tl.strip() for tl in file.readlines()]\n",
    "  d, e = parse(textlines, subtask = 1)\n",
    "  data.extend([x for x in d if x[\"date\"][0]!=None])\n",
    "  print(\"Errors:\", e)    \n",
    "with open('data/original/moretraining/training12T2.txt', 'r') as file:\n",
    "  textlines = [tl.strip() for tl in file.readlines()]\n",
    "  d, e = parse(textlines, subtask = 2)\n",
    "  data.extend([x for x in d if x[\"date\"][0]!=None])\n",
    "  print(\"Errors:\", e)\n",
    "\n",
    "for i in range(len(data)):\n",
    "  data[i]['tokens'] = [sentence.strip().split() for sentence in data[i]['text'].split(\".\") if len(sentence) > 0]\n",
    "  assert data[i]['date'][0] <= data[i]['date'][1]\n",
    "\n",
    "print(\"Found %i data items\" % len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yW2iHLUjo8wJ"
   },
   "outputs": [],
   "source": [
    "fn = os.path.join('data', 'SemEval2015.npz')\n",
    "np.savez_compressed(fn, data=data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WpfyeAhuHnPd"
   },
   "source": [
    "## Colonia\n",
    "\n",
    "Corpus of Historical Portuguese\n",
    "\n",
    "__TODO:__\n",
    "* Verify that all files are found\n",
    "* Check for duplicates\n",
    "* Check for empty of suspiciously short books"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "svdUp0FYIErY",
    "outputId": "be667cf8-270f-4e33-96ef-d08e57082d40"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 103 files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading colonia: 100%|██████████| 4/4 [00:00<00:00, 41.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ", 4 missing files\n"
     ]
    }
   ],
   "source": [
    "# This page links to the source material\n",
    "with urlopen(\"\"\"http://corporavm.uni-koeln.de/colonia/inventory.html\"\"\") as source:\n",
    "    data = source.read().decode(errors='replace')\n",
    "\n",
    "base_url = \"\"\"http://corporavm.uni-koeln.de/colonia/\"\"\"\n",
    "data_path = os.path.join('data', 'original', 'colonia')\n",
    "\n",
    "# Parse out files from the web page\n",
    "pattern = r'<a\\W+href=\\\"([\\w\\/]+.txt)\\\">'\n",
    "files_in_html = re.findall(pattern, data, flags=re.IGNORECASE+re.MULTILINE)\n",
    "file_urls = [base_url+fn for fn in files_in_html]\n",
    "files = [tuple([url, os.path.join(data_path, url.split(\"/\")[-1])]) for url in file_urls]\n",
    "print(\"Found %i files\" % len(files))\n",
    "\n",
    "# Make data path\n",
    "if not os.path.exists(data_path):\n",
    "    os.makedirs(data_path)\n",
    "\n",
    "def _get(data):\n",
    "    \"\"\"Simple http downloader\"\"\"\n",
    "    url, local_fn = data\n",
    "    from urllib.request import urlretrieve\n",
    "    try:\n",
    "        urlretrieve(url, local_fn)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "with Pool(processes=max(5, os.cpu_count())) as pool:\n",
    "    # Only download files once\n",
    "    dl = [f for f in files if not os.path.exists(f[1])]\n",
    "    # The actual downloading\n",
    "    for data in tqdm(pool.imap_unordered(_get, dl), desc=\"Downloading colonia\", total=len(dl)):\n",
    "        pass\n",
    "\n",
    "# Count the missing files\n",
    "missing = [f for f in files if not os.path.exists(f[1])]\n",
    "if len(missing) > 0:\n",
    "    print(\", %i missing files\" % len(missing))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 425
    },
    "colab_type": "code",
    "id": "nP2eql2oa41t",
    "outputId": "91a278f8-51df-4e5c-9d0a-ecfae9435b71"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "﻿<text id=\"melo1650\">\n",
      "<s>\n",
      "Pinto\tV\tpintar\n",
      "para\tPRP\tpara\n",
      "os\tDET\to\n",
      "tempos\tNOM\ttempo\n",
      "a\tPRP\ta\n",
      "imagem\tNOM\timagem\n",
      "de\tPRP\tde\n",
      "um\tDET\tum\n",
      "abreu1856.txt\t  barbosa1691.txt   guerreiro16th.txt  queiroz1878.txt\n",
      "aires1752.txt\t  barreto1915.txt   holanda1548.txt    queiroz1887.txt\n",
      "alencar1857.txt   barreto1920.txt   lobo1619.txt       queiroz1888.txt\n",
      "alencar1862.txt   barreto1948.txt   macedo1811.txt     queiroz1900.txt\n",
      "alencar1865.txt   barros1540.txt    macedo1844.txt     rocha1910.txt\n",
      "alencar1875.txt   botelho1705.txt   macedo1878.txt     sanches1760.txt\n",
      "almeida1633.txt   brandao1632.txt   machado1876.txt    silva1733.txt\n",
      "almeida17th.txt   brochado17th.txt  machado1878.txt    silva1734.txt\n",
      "almeida1852.txt   caminha1500.txt   machado1881.txt    silva1735.txt\n",
      "almeida1901.txt   caminha1893.txt   machado1885.txt    silva1736b.txt\n",
      "almeida1905.txt   caminha1894.txt   machado1891.txt    silva1736.txt\n",
      "almeida1921.txt   caminha1895.txt   machado1899.txt    silva1737b.txt\n",
      "alves1870.txt\t  caminha1896.txt   machado1904.txt    silva1737.txt\n",
      "anchieta1560.txt  camoes1572.txt    machado1908.txt    silva1738.txt\n",
      "anchieta1586.txt  camoes16th.txt    machado1927.txt    sousa1556.txt\n",
      "andrade1928.txt   chagas1687.txt    machado1936.txt    taunay1872.txt\n",
      "azevedo1855.txt   costa1652.txt     matos17th1.txt     verney1746.txt\n",
      "azevedo1882b.txt  cunha1902.txt     matos17th2.txt     vicente16th.txt\n",
      "azevedo1882.txt   dumont1918.txt    melo1650.txt       vieira1640.txt\n",
      "azevedo1884b.txt  faria16th.txt     nabuco1883.txt     vieira1642.txt\n",
      "azevedo1884.txt   galhegos1641.txt  ortas1777.txt      vieira1655.txt\n",
      "azevedo1890.txt   gandavo1576.txt   pinto1510.txt      vieira1657.txt\n",
      "azevedo1894.txt   garcao18th.txt    pompeia1888.txt    vieira1670.txt\n",
      "azevedo1895.txt   garrett1846.txt   queiroz1875.txt    vieira17th.txt\n"
     ]
    }
   ],
   "source": [
    "!head -n 10 data/original/colonia/melo1650.txt\n",
    "!ls data/original/colonia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZzNN_E50ug5j"
   },
   "source": [
    "The following is some old code for processing Colonia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "ffNOH9rveKnt",
    "outputId": "c75500de-da44-4217-fbc6-d2fe61872de7"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing Colonia: 100%|██████████| 99/99 [00:03<00:00, 31.61it/s]\n"
     ]
    }
   ],
   "source": [
    "colonia = list()\n",
    "year_pattern = r'[\\w\\/]+((1[5-9])([0-9]{2}|th))[ab12]?.txt'\n",
    "\n",
    "def _parse_file(fn):\n",
    "  import re\n",
    "  groups = re.match(year_pattern, fn.split(\"/\")[-1]).groups()\n",
    "  if groups[2].isnumeric():\n",
    "    years = int(groups[0])\n",
    "  else:\n",
    "    c = int(groups[1])*100\n",
    "    years = tuple([c, c+99])\n",
    "  # print(years)\n",
    "  with open(fn, 'r', encoding='utf-8', errors='replace') as f:\n",
    "    tokens = list()\n",
    "    pos = list()\n",
    "    lemmas = list()  \n",
    "    in_sentence = False\n",
    "    for n, rawline in enumerate(f.readlines()):\n",
    "      text = rawline.strip()\n",
    "      if text.find(\"<s>\") >= 0:\n",
    "        tokens.append([])\n",
    "        pos.append([])\n",
    "        lemmas.append([])\n",
    "        in_sentence = True\n",
    "      elif text.find(\"</s>\") >= 0:\n",
    "        in_sentence = False\n",
    "      elif re.search(r'<[0-9]+>', text):\n",
    "        # Ignore weird tokens\n",
    "        pass\n",
    "      elif in_sentence:\n",
    "        try:\n",
    "          token, part, lemma = text.split()\n",
    "          if lemma == \"@card@\":\n",
    "            token = \"<NUM>\" # Remove number tokens\n",
    "          tokens[-1].append(token)\n",
    "          pos[-1].append(part)\n",
    "          lemmas[-1].append(lemma)\n",
    "        except:\n",
    "          pass\n",
    "          # print(\"Error in %s on line %i: %s\" % (fn.split(\"/\")[-1], n, text))\n",
    "  return {'date': years, 'file': fn, 'tokens': tokens, 'pos': pos, 'lemmas': lemmas}\n",
    "\n",
    "with Pool(processes=os.cpu_count()) as pool:\n",
    "  files_for_parsing = [f[1] for f in files if os.path.exists(f[1])]\n",
    "  for d in tqdm(pool.imap_unordered(_parse_file, files_for_parsing),\n",
    "                   desc=\"Parsing Colonia\", total=len(files_for_parsing)):\n",
    "    colonia.append(d)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ylyfABz1s9e9"
   },
   "outputs": [],
   "source": [
    "fn = os.path.join('data', 'Colonia.npz')\n",
    "np.savez_compressed(fn, data=colonia)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hNT_eJCcxOYZ"
   },
   "source": [
    "## SDHK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "colab_type": "code",
    "id": "OKBoFZU7xOha",
    "outputId": "a3766792-db9e-46be-ba1e-e31a1bc9dda9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3086 charters with Swedish\n",
      "Found 7572 charters with Latin\n"
     ]
    }
   ],
   "source": [
    "urlretrieve(\"https://raw.githubusercontent.com/fredrikwahlberg/harvesters/master/sdhk.py\", \n",
    "            \"external/sdhk.py\")\n",
    "from sdhk import SDHKHarvester\n",
    "\n",
    "db = SDHKHarvester(os.path.join('data', 'original', 'sdhk.json.gz'))\n",
    "\n",
    "ids_with_textcontent = set([n for n in db.get_good_ids()\n",
    "                            if 'textcontent' in db[n].keys() and\n",
    "                            db[n]['textcontent'] is not None and\n",
    "                            len(db[n]['textcontent']) > 100])\n",
    "ids_with_year = set([n for n in db.get_good_ids() if 'year' in db[n].keys() and\n",
    "                     db[n]['year'] >= 1100 and\n",
    "                     db[n]['year'] <= 1523])\n",
    "\n",
    "ids_with_swedish = set([n for n in db.get_good_ids()\n",
    "                        if 'language' in db[n].keys() and\n",
    "                        str(db[n]['language']).find('svenska') >= 0])\n",
    "good_ids_swedish = list(ids_with_swedish\n",
    "                .intersection(ids_with_year)\n",
    "                .intersection(ids_with_textcontent))\n",
    "print(\"Found %i charters with Swedish\" % len(good_ids_swedish))\n",
    "\n",
    "ids_with_latin = set([n for n in db.get_good_ids()\n",
    "                        if 'language' in db[n].keys() and\n",
    "                        str(db[n]['language']).find('latin') >= 0])\n",
    "good_ids_latin = list(ids_with_latin.intersection(ids_with_year).intersection(ids_with_textcontent))\n",
    "print(\"Found %i charters with Latin\" % len(good_ids_latin))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HE2D-44ltlP5"
   },
   "source": [
    "### Swedish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1rL5rLJLyDKF"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3086/3086 [00:13<00:00, 234.96it/s]\n"
     ]
    }
   ],
   "source": [
    "data = list()\n",
    "for i in tqdm(good_ids_swedish):\n",
    "    data.append(db[i])\n",
    "    #data[-1]['text'] = re.sub('\\W+', ' ', data[-1]['textcontent'].lower())\n",
    "    data[-1]['text'] = textprocess.transform(data[-1]['textcontent'])\n",
    "    data[-1]['date'] = data[-1]['year']\n",
    "    #data[-1]['tokens'] = [sentence.split() for sentence in data[-1]['textcontent'].split(\".\") if len(sentence) > 0]\n",
    "    data[-1]['tokens'] = [sentence.split() for sentence in data[-1]['text'].split(\".\") if len(sentence) > 0]\n",
    "\n",
    "fn = os.path.join('data', 'SDHK_Swedish.npz')\n",
    "np.savez_compressed(fn, data=data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QoaikxrctmPA"
   },
   "source": [
    "### Latin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "L4jKeuQA0vXK"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7572/7572 [00:31<00:00, 238.52it/s]\n"
     ]
    }
   ],
   "source": [
    "data = list()\n",
    "for i in tqdm(good_ids_latin):\n",
    "    data.append(db[i])\n",
    "    #data[-1]['text'] = re.sub('\\W+', ' ', data[-1]['textcontent'].lower())\n",
    "    data[-1]['text'] = textprocess.transform(data[-1]['textcontent'])    \n",
    "    data[-1]['date'] = data[-1]['year']\n",
    "    #data[-1]['tokens'] = [sentence.split() for sentence in data[-1]['textcontent'].split(\".\") if len(sentence) > 0]\n",
    "    data[-1]['tokens'] = [sentence.split() for sentence in data[-1]['text'].split(\".\") if len(sentence) > 0]\n",
    "\n",
    "fn = os.path.join('data', 'SDHK_Latin.npz')\n",
    "np.savez_compressed(fn, data=data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# St. Clare Archive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import zipfile\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Diplomatic level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "level = \"dipl\"\n",
    "data_path = os.path.join('data', \"original\", \"StClare_{}.zip\".format(level))\n",
    "zip_archive = zipfile.ZipFile(data_path, \"r\")\n",
    "\n",
    "velux = pd.read_csv(zip_archive.open(os.path.join(\"StClare_{}\".format(level), \"meta.csv\")), sep=\"\\t\", header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'lat': 361, 'dan': 100, 'swe': 6, 'mlg': 2})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(velux[\"language\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 469/469 [00:02<00:00, 224.16it/s]\n"
     ]
    }
   ],
   "source": [
    "velux_data =  list()\n",
    "undated = list()\n",
    "for i, doc in tqdm(velux.iterrows(), total=len(velux.index)):\n",
    "    \n",
    "    doc_path = os.path.join(\"StClare_{}\".format(level), \"{}.txt\".format(int(doc[\"text number\"])))\n",
    "    \n",
    "    with zip_archive.open(doc_path, \"r\") as f:\n",
    "        content = f.read().decode(\"utf-8\").strip()\n",
    "        content = textprocess.transform(content)\n",
    "\n",
    "    data = {\"id\":   doc[\"text number\"],\n",
    "            \"date\": (doc[\"year-min\"], doc[\"year-max\"]),\n",
    "            \"text\": content,\n",
    "            \"language\": doc[\"language\"],\n",
    "            \"tokens\": [content.split(), ]\n",
    "           }\n",
    "\n",
    "    if pd.isnull(doc[\"year-min\"]):\n",
    "        undated.append(data)\n",
    "\n",
    "    else:\n",
    "        velux_data.append(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "dest_latin = os.path.join('data', \"StClare_{level}_latin.npz\".format(level=level))\n",
    "np.savez_compressed(dest_latin, \n",
    "                    data=list(filter(lambda x: x[\"language\"]==\"lat\", velux_data)), \n",
    "                    undated=list(filter(lambda x: x[\"language\"]==\"lat\", undated)))\n",
    "\n",
    "dest_danish = os.path.join('data', \"StClare_{level}_danish.npz\".format(level=level))\n",
    "np.savez_compressed(dest_danish, \n",
    "                    data=list(filter(lambda x: x[\"language\"]==\"dan\", velux_data)),\n",
    "                    undated=list(filter(lambda x: x[\"language\"]==\"dan\", undated)))\n",
    "\n",
    "#dest_misc = os.path.join(base_path, \"velux_{level}_misc.npz\".format(level=level))\n",
    "#np.savez_compressed(dest_misc, \n",
    "#                    data=list(filter(lambda x: x[\"language\"] not in (\"dan\", \"lat\"), velux_data)),\n",
    "#                    undated=list(filter(lambda x: x[\"language\"] not in (\"dan\", \"lat\"), undated)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Facsimile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "level = \"facs\"\n",
    "data_path = os.path.join('data', \"original\", \"StClare_{}.zip\".format(level))\n",
    "zip_archive = zipfile.ZipFile(data_path, \"r\")\n",
    "\n",
    "velux = pd.read_csv(zip_archive.open(os.path.join(\"StClare_{}\".format(level), \"meta.csv\")), sep=\"\\t\", header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 469/469 [00:02<00:00, 179.82it/s]\n"
     ]
    }
   ],
   "source": [
    "velux_data =  list()\n",
    "undated = list()\n",
    "for i, doc in tqdm(velux.iterrows(), total=len(velux.index)):\n",
    "    \n",
    "    doc_path = os.path.join(\"StClare_{}\".format(level), \"{}.txt\".format(int(doc[\"text number\"])))\n",
    "    \n",
    "    with zip_archive.open(doc_path, \"r\") as f:\n",
    "        content = f.read().decode(\"utf-8\").strip()\n",
    "        content = textprocess.transform(content)\n",
    "\n",
    "    data = {\"id\":   doc[\"text number\"],\n",
    "            \"date\": (doc[\"year-min\"], doc[\"year-max\"]),\n",
    "            \"text\": content,\n",
    "            \"language\": doc[\"language\"],\n",
    "            \"tokens\": [content.split(), ]\n",
    "           }\n",
    "\n",
    "    if pd.isnull(doc[\"year-min\"]):\n",
    "        undated.append(data)\n",
    "\n",
    "    else:\n",
    "        velux_data.append(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "dest_latin = os.path.join('data', \"StClare_{level}_latin.npz\".format(level=level))\n",
    "np.savez_compressed(dest_latin, \n",
    "                    data=list(filter(lambda x: x[\"language\"]==\"lat\", velux_data)), \n",
    "                    undated=list(filter(lambda x: x[\"language\"]==\"lat\", undated)))\n",
    "\n",
    "dest_danish = os.path.join('data', \"StClare_{level}_danish.npz\".format(level=level))\n",
    "np.savez_compressed(dest_danish, \n",
    "                    data=list(filter(lambda x: x[\"language\"]==\"dan\", velux_data)),\n",
    "                    undated=list(filter(lambda x: x[\"language\"]==\"dan\", undated)))\n",
    "\n",
    "#dest_misc = os.path.join(base_path, \"velux_{level}_misc.npz\".format(level=level))\n",
    "#np.savez_compressed(dest_misc, \n",
    "#                    data=list(filter(lambda x: x[\"language\"] not in (\"dan\", \"lat\"), velux_data)),\n",
    "#                    undated=list(filter(lambda x: x[\"language\"] not in (\"dan\", \"lat\"), undated)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ... and lastly\n",
    "\n",
    "Some code for loading the created data sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "bxNG0ttAFXmN",
    "outputId": "397116f8-d177-4a05-ed47-6f92c3caa7a3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data/StClare_facs_danish.npz',\n",
       " 'data/SDHK_Latin.npz',\n",
       " 'data/StClare_facs_latin.npz',\n",
       " 'data/StClare_dipl_danish.npz',\n",
       " 'data/StClare_dipl_latin.npz',\n",
       " 'data/velux_facs_danish.npz',\n",
       " 'data/SDHK_Swedish.npz',\n",
       " 'data/Colonia.npz',\n",
       " 'data/SemEval2015.npz',\n",
       " 'data/velux_dipl_latin.npz',\n",
       " 'data/velux_facs_latin.npz',\n",
       " 'data/velux_dipl_danish.npz']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def load_dataset(data_source_filename):\n",
    "    print(\"Loading %s... \" % data_source_filename.split(\"/\")[-1], end=\"\")\n",
    "    t = time.time()\n",
    "    dataset = dict()\n",
    "    with np.load(data_source_filename, allow_pickle=True) as source_file:\n",
    "        for key in source_file.keys():\n",
    "            # print(key)\n",
    "            dataset[key] = source_file[key].tolist()\n",
    "    print(\"done (%.1fs)\" % (time.time()-t), flush=True)\n",
    "    return dataset\n",
    "\n",
    "data_source_filenames = [os.path.join('data', fn) for fn in os.listdir('data')\n",
    "                            if os.path.isfile(os.path.join('data', fn)) and fn[-3:]=='npz']\n",
    "data_source_filenames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verify that texts looks good after preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading StClare_facs_danish.npz... done (0.0s)\n",
      "Alle mendt thꝫͤ bꝛeff ee ell̅ꝛ hø læſ / helſſze vij p̲ moge̅ſ i vgliidſtͮp hꝛitz ffogett i waalbuꝛgꝭ hꝛiitt / Olűff mi i oꝛdeͮp / knd vgle j kaꝛlleby / Ewi̅deliighe mett gd / kngø vij ffoꝛ alle mett Thꝫͤ voꝛtt opne bff / Anno dn̅j <NUM> / Thn̅ løffwdag neſt ffaa ȷomffͮ mae dag natiͭꝭ Tha voꝛ / ſkiickett ffoꝛ oſſ oc ma̅ge da̅ne mend fle paa waalbuꝛgꝭ hꝛii ti̅ng / Eꝛliig oc ffoꝛnw̅ftiige mand / han lock i abbeted / paa ffͮ cꝛeſtenſ vegne j kla / oc haffde tiiſſzͤ effthꝛ̅ ſkꝭᷠͤ <NUM> da̅ne mend mett ſſeeg / ſo woꝛ / han dÿꝛiickſ j aaby veſt / laűn hanſ ibi / niel villomſ ibi p̲ ȷepſ i hoꝛſſzeſtaal / ȷond olſ i toꝛckiilſtͮp / niel enſ i toꝛckiilſtͮp / ȷngoꝛ hanſ i kiiꝛke ſaaby Tiilløff p̲ſ ibi / hilke ffoꝛne <NUM> da̅ne mend ſſo tiil waatagne ȷndhn̅ ti̅nghe / tiil thn̅ aaſynd paa thn̅ ſkoſſ lood ſo liighꝛ ⸌ tiil ⸍ niel teſ gaadt i foꝛᷠͤ toꝛckıılſtͮp / the ſſaade oc ſkdde o hand voꝛ god foꝛ oldhn̅ ſkooff / o bondhn̅ ſo booꝛ / paa boole knde fꝛij thꝛ nogle ind / thꝛ paa ellꝛ ey paa ind hoſbon vegne / a̅mele ſa̅me foꝛſkꝭᷠͤ dag tha fꝛe gi̅nghe ȷndhn̅ ti̅nghe Tiiſſze effthꝛ ſkꝭᷠͤ <NUM> da̅neme̅ / oc ſſaa dett aff ffoꝛ ꝛetthe / Att bondhn̅ ſo booꝛ paa boole maa haffe ſſaa manghe ind ffꝛij paa ſind ſkoff ſo liighꝛ tiil foꝛᷠͤ nielſſ ⸌ teſ⸍ gaaꝛdt paa ſind hoſbon vegne / ſſo ha̅ kand holle paa boole / paa tiiſſze ooꝛ oc aꝛtiickle { ken} 00000 han lock ett vűiilt tingꝭ vidne aff <NUM> tꝛoffaſthe da̅ne mend / Tha tiil meltꝭ føꝛſt ȷøꝛghn̅ ȷenſ j Tyde Att ha̅ ſklle tiil ſeeg tage <NUM> da̅ne mend / ſſo voꝛ / oluff p̲ſ i lynby / niel p̲ſ ibi / oc mogh̅n andſ ibi / han ipſ ibi / niel møꝛckeꝛ i kyndeløſſze / nielſſ olſ i nøꝛꝛ holſſøø oc ȷøꝛghn̅ nielſ i enſløff / laűn ȷenſ ı liille kaꝛlleby / laűn nielſ i egby / mattꝭ die̅g j aaby nielſſ laűnſ i kyndeløſſze / hilke ffoꝛᷠͤ <NUM> da̅ne mend / vd gi̅nghe ı beꝛaad alle ȷnd ige kom̅er velbeͦꝛede alle { an}deteliighe vnde paa thꝛꝛꝭ godhe tꝛo ſieel oc ſſandhꝭ liighꝛ iſt j alle maade ſſo tiiſſze <NUM> ſkꝭᷠͤ da̅ne mend haffe vndiitt ffoꝛ oſſ ſaa viidne oc / vij Alle effthꝛ th̅ / Att ffoꝛᷠͤ bonde ſſo paa boole booꝛ / maa haffe ſſaa ma̅ghe wind fꝛij paa ſynd ſkooff ſſo ffoꝛſkꝭͭ ſtandhꝛ paa ind hoſſbon / Alle ooꝛ oc Aꝛtiickle j alle maa¦dhe ſo foꝛſkꝭͭ ſtandhꝛ / Att ſaa j ſſandhett eꝛ / Thꝫ viidne vij mett vaa ȷndſſegle henghn̅¦ne nædhn̅ ffaa Thꝫͤ voꝛtt opne bꝛeff giiffett aaꝛ oc dag ſthed oc ſtűnd ſo foꝛſkꝭͭ ſtaꝛ\n",
      "\n",
      "Loading SDHK_Latin.npz... done (1.2s)\n",
      "Omnibus presentes litteras visituris [ d.v.s . : visuris ] vel audituris , Tuwo viteadnctis [ d.v.s . viceaduocatus ] stokholmensis , et chonradus arxø , conciuis ibidem . salutem in domino sempiternam . Nouerint vniuersi pos reuerendum dominum et illustrem dominum , laurencium vlphsson legiferum sudermannie ab omnibus oneribus debitorum in quibus peggenborg , et heredibus suis obligatus tenebatur quittum et liberum penitus dimisisse . Jn cuius rei testimonium , sigilla nostra presentibus sunt appensa . Scriptum Anno domini M . CCC . XXX . sexto . die sanctorum martyrum viti et modesti .\n",
      "\n",
      "Loading StClare_facs_latin.npz... done (0.0s)\n",
      "Om̅ıbꝫ pn̅ ſcptu̅ cnentıbꝫ Nıcholau andthoꝛp l̅t ín dn̅o / Nou͛ínt ỽníu͛¦ſí ꝙ recognoſco / me / ín boní ſoꝛoꝝ ſc̅e claꝛe de roſꝃ ín ıæſſhøghæ  lí locí ı̅ walbuſhæ ỽl̅ Hoꝛnſhæ ſıtí / níl íurí hab͛e ſꝫ debeo ıp̅a bon eıſ ſoꝛoꝛíbꝫ q̅ncu̅qꝫ ỽolu͛ínt / lıb͛e reſígnare / lr̅a qꝫ ıp̅aꝝ  ſup̲ eıſ boní data / nll̅ıu e̅e ỽıgoꝛí ỽl̅ ỽaloꝛí In . cuıꝰ Re ᴛeﬅı̅o ſıgıllu̅ meu̅ pn̅tıbꝫ e̅ ae̅ſu̅ Datu̅ a̅no dn̅ <NUM> <NUM> <NUM> In díe ſc̅ı olauí Regꝭ  mrtꝭ\n",
      "\n",
      "Loading StClare_dipl_danish.npz... done (0.0s)\n",
      "Jegh søsther bouell bodz dotther abedisse i klare klosther wtj roskyld och meninge kowentt y samme sted kendis wy oc witerlige giøre met thette vortt obne breff att wy haffue wntt oc for lentt oc met thette wortt obne breff wndher oc for lenner erlig oc welbyrdigh mand erich christoffersøn aff windinge oc hans kerer hwstrv frw Sysille mauris skaffues datther at wor oc klosthers gods som lyger tiill abedissen i klare kloster y flacke bieres herret y begis theris lyuefs tiid som her effter føller først <NUM> gorde i Snessluff y ! them ¡ første boer lauris morthensøn y ! them ¡ ! andhen ¡ bouer lauris ! mortemsøn ¡ Swogher y ! them ¡ tredie lile ! morthem ¡ y ! them ¡ fiere ! moghems ¡ brabe och gyffuer thy alle sammenn <NUM> pund kornn <NUM> skilling grott arbede peninge <NUM> gord i høffue som morthen mwrer i bouer och gyffuer <NUM> pund bygh oc <NUM> pund rugh <NUM> skilling grott <NUM> gord i liunghby som hedher iens troelsøn i bouer gyffuer <NUM> pund ! kormn ¡ <NUM> bygh oc halt rugh <NUM> skilling <NUM> gord i regnerstrvp som hans ogesøn y bouer gyffuer <NUM> pund byg och <NUM> ørtug rug <NUM> skilling <NUM> gord y øllervp som iep persøn i boer gyffuer <NUM> pund rug <NUM> pund byg <NUM> skilling grott <NUM> gord i høghe bieregh gyffuer <NUM> skilling grott tesse for screffne gorde oc klosters gorde skall for neffde erich christo¦ffersøn oc hans hustrv y begis theris liuess tiid met ! sodamn ¡ skeell och for oer att hand skall lade bønderne ydhe woss langillet y Roskyld eller huar wy wille haffuet indhen kødemøsse tiill gode rede oc for swore bønderne oc holle them wed loff oc skeell oc skall for neffde erich christoffersøn nyde gester{j } oc halffdelig sage faell oc gord festning oc halffdelig skall komme woss tiill gode thiiss ødermere beuissniegh oc bedher for voring henge wy couentz indzelle nedhen for thette wor obne breff Screffuitt vtj roskyld filipe et iacobi dagh Anno dominj <NUM>\n",
      "\n",
      "Loading StClare_dipl_latin.npz... done (0.0s)\n",
      "Omnibus hoc scriptum cernentibus / Iacobus filius comitis Nicholai . salutem in domino . Constare volumus vniuersis nos / religiosis sororibus sancte Clare roskildis inclusis / vendidisse omnia bona nostra mobilia et in¦mobilia in kyndæsløf / pro quibus nobis totum pretium promissum / est ad nostre beneplacita voluntatis jntegraliter persolutum / vnde nos substituentes latorem presencium nicholaum mandorp ad scotandum dictis sororibus dicta bona ex parte nostra ratum habemus et firmum quicquid idem . Nicholaus . ex parte nostra super scotacione eorundem duxerit ordinandum . In cuius rei testimonium sigillum nostrum pre¦sentibus litteris est appensum . Datum anno domini millesimo . <NUM> . <NUM> octauo in festo sanctorum nerei et achillis\n",
      "\n",
      "Loading velux_facs_danish.npz... done (0.0s)\n",
      "Wıȷ frederıch mett gud nade wtuold konnı̅g tıl danmarck rett arffwı̅ng tıll Norge Htıng ı leſŭıck holﬅen̅ tormar̅n oc Dıtmerſch̅n Greﬀwe ı oldenborg og delmenhorﬅ Gıør̅ alle wıtth̅rlıgt atth aar eﬀth̅ꝛ guds byrd NUMBER_TOKEN th̅n Logerdag neﬅ for S̅ctı Laurentij martırı dag wtij wor Stad Roſkıld nerŭerın o elꝭͤ Her Moens gøye wor og danmarckꝭ Rııgꝭ hoﬀmeﬅh̅ꝛ Her hen¦rıch krvmedıcke ridder̅ oc oluﬀ mel wor̅ mend oc Raad wor ſkıckett o elꝭͤ Tønne tønſ wor mand oc tıener paa th̅n ene oc haﬀde ı retthe ſteffnd Han ol wor oc kronen bvnde i lyndhe paa then̅ andh̅n ſıdhe for et ⸠ ſtycke ⸡ ⸌ fıerding ⸍ ıord paa lynde marck ſo forᷠͤ hans ol ſagde at neffnı̅ghe ı faxe hret haﬀde hanno̅ tıılfvndet haﬀde for th̅n brøﬅ hand ſagde ſeg att haffŭe ı ſıt rett moll ıord paa forᷠͤ linde marck ſo hand for o beŭiﬅ mett et opett bezeglett tingꝭ¦winne aﬀ faxe herrꝭ ting Th̅ꝛ tiil ſwarede forᷠͤ Tonne ſagde oc bevıﬅhe met leﬀuen mantz røﬅ att ſam̅e ıord haﬀde wær̅t tııll Sancte klar̅ kloﬅ̅ꝛ ı Roſkıld ! wilſket ¡ oc wkerd ſaa lenghe noger man̅d lengﬅ myn kŭnde oc forﬅ for̅ o ı rette lagdhe ett oﬀŭett ⸠ be ⸠ bezeglet pergmantzbreﬀ lyde̅ at e riddermantzman̅d hed Per ol ı kalrii gord haﬀde giﬀuett sam̅e ⸠ ﬅycke ⸡ ⸌ fıerding ⸍ ıord tııll forᷠͤ S̅cte klar̅ kloﬅer ı Roıld for ſıne oc ſıne forelder ſielle Oc beꝛette forᷠͤ Tønne Tønneſ ad ſa̅me neﬀninge haﬀde fvnnet forᷠͤ ıord tiill hans olſens gord for hog¦borne fyrﬅꝭ konnı̅g Chrııﬅıern friicth oc far̅ ſkyld ſo ſamme neffnige ſa̅me tiid for o tııll ﬅode ⁊ cᷓ Mett fler̅ ord ſo th̅ꝛ o paa ſam̅e tııd paa bode ſiidh̅ꝛ emello løbe Tha eﬀth̅ꝛ tııltall genſwar breﬀŭe beuiſenı̅g oc leffuende mandz røﬅ ſo tha for tılﬅede wor Wortt th̅ꝛ ſaa paa ſagt for̅ rette ad forᷠͤ ıord ſkall blıffue tııll forᷠͤ S̅cte klar̅ cloﬅer ſo hv̅ aﬀ arıltztıd wærett haffuer Oc hue broſt forᷠͤ han ol haﬀuer ı ſıth moll ſkall hand talle alle lotzer̅er̅ tıll o hanno̅ ycke nogꝭ Giffuet aar dag oc ﬅedt ſo forneŭ̅ett ﬅor̅ Wnder Wort Sıgnetꝭ Ad ma̅tu̅ dn̅ȷ Regꝭ ꝓpu̅\n",
      "\n",
      "Loading SDHK_Swedish.npz... done (0.3s)\n",
      "Alla the mæn thetta breff høra ella see helsar jak Nisse Knwtson æwerdhelika meth Gudh . Thet skal allom mannom viterlikt wara , thet jak hawer vnt ok vplatit Nydal clostre , vndir conuentonna bordh , en gardh i Bryne innan Angerdhaheyster sokn liggiande , i Moohæradhe , for mit brodherskap ok minna hustrv Ingegerdha Orniolfsdotters ok for wara æwerdhelika siæla røkt ok wara forældra , huilkin gardh mik staar til pant aff herra Joan Vddason for xxx mark redho pæninga . Kan thet swa wara , at nokar man aff herra Joan Vddassons arffwa wil thet fornempda goz i Bryne aterløsa for the fornempda xxx mark , tha giwi conuentonne i Nydal fornempda xxx mark pæninga , tholika tha ganga ok gilde æru i rikeno , ok taki ater gozit . Til witnisbyrdh ok bætre forwaring hawer jak fornempde Nisse Knutson mit incigle henkt for thetta breff ok bidhir jak erlika mæn , swa som ær Aruidh Jønsson , Anund Hemminxson , Andris Knutson , ok Throtte Attasson , hærezhøffdhingen i Ostbo , at the sætin siin incigle her fore meth til witnisbyrdh . Scriptum Kælunda , anno Domini m°cccc°xij , feria secunda proxima ante festum sancti Botolfi abbatis .\n",
      "\n",
      "Loading Colonia.npz... done (5.4s)\n",
      "<generator object <genexpr> at 0x7fc25ca0b950>\n",
      "\n",
      "Loading SemEval2015.npz... done (0.1s)\n",
      "Rodgers'z famous Lozenges for the Heartburn in two or three Minutes , in order to prevent Counterfeits being impos'd upon Clement's Churchyard in the Strand , ready sealed up , upon the Publick .\n",
      "\n",
      "Loading velux_dipl_latin.npz... done (0.1s)\n",
      "Om(n)ibus p(rese)ns sc(ri)ptu(m ) c(er)nentib(us ) / Petrus grubbæ de oræbyærgh / Sal(u)t(e)m in d(omi)no sempit(er)na(m ) . Not(um ) facio vniu(er)sis / q(uod ) molendinu(m ) q(uo)ddam / ap(ud ) thorslundæ litlæ in litlæh(æræt ) / sit(um ) / / q(uo)nda(m ) he(m)mingo pæt(er)s(un ) bone memorie p(at)ri dil(e)c(t)e ( con)sortis mee / p(er ) pet(ru)m d(i)c(tu)m snubbæ / p(ro ) ducentis ( et ) octoginta m(a)r(chis ) den(ariorum ) vsual(is ) monete syellenden(sis ) impign(er)at(um ) / placito litlæh(æræt ) p(ro)xi(mo ) an(te ) die(m ) b(eat)ei nicholai anni cui(us)cu(n)q(ue ) Redi¦m(en)du(m ) quod q(ui)dem molendinu(m ) boeci(us ) dyræ cui(us ) a(n)i(m)e ! p(ro)piciet(ur ) ¡ / de(us ) dil(e)c(t)e mee vxoris marit(us ) d(omi)no Ebboni Joh(ann)is canonico Roskilden(si ) / in Reco(m)pensam p(ro ) bonis suis in Ringsbyærgh byæu(er)¦scoghsh(æræt ) / Ret(ri)buit ( et ) scotauit / Exhi(bi)tori p(re)nc(ium ) Joh(ann)i magnuss(un ) / ad man(us ) suas c(um ) iurib(us ) ( et ) causis q(ui)b(us ) m(ihi ) stare dinoscit(ur ) dimitto ( et ) Resigno p(er ) p(rese)ntes / p(ro ) suis vsib(us ) ( et ) h(er)ed(um ) suor(um ) lib(er)e ordinandu(m ) / Jn cui(us ) Rei testimo(nium ) sig(i)ll(u)m meu(m ) p(rese)ntib(us ) est appensum . dat(um ) a(n)no d(omi)ni NUMBER_TOKEN . NUMBER_TOKEN . NUMBER_TOKEN . sexto / sabb(at)o p(ro)ximo post octaua(m ) corp(or)is ( Christ)i .\n",
      "\n",
      "Loading velux_facs_latin.npz... done (0.1s)\n",
      "Om̅ıbꝫ pn̅ ſcptu̅ c͛nentıbꝫ Soꝛoꝛ Cecılıa herlugſdot͛ poꝛıſſ p ſc̅a gnete̅ Roſꝃ ᴛotꝰqꝫ co̅uentuſ ı c f Ioh̅e ıcíu pꝛíoꝛ c ꝓuıſoꝛ eɼu̅de sl̅t in dn̅o . Nou͛í¦tı ꝙ no de ỽnním ỽoto oı̅ nr̅ Rtıfıcamꝰ gr̅a q olı̅ nobl̅ı dn̅e c ſoꝛoꝛí nr̅e Cɼíﬅíne fılıe dn̅ Ioh̅ı ſyelenſfaræ ſenioꝛí ꝯceſſímꝰ . ỽt ỽıdelꝫ ı̅ ỽıta  ı̅ moꝛte s lıc͛et de ɼebꝫ ſui lıb͛e oꝛdı̅re  ꝓ ı̅a ſu dıſpon͛e c lrgırí . Oblıga̅te no fírmıt͛ p̲ pn̅¦te ꝙ íux deſıgnt̅oe  exp̅ſſıone̅ eíuſ dn̅e  ſoꝛoꝛi nɼ̅e fc̅a in extremi / debeꝰ bſqꝫ díminuc̅oe qlıbꝫ ſua̅ ỽolu̅tate̅ extɼema̅ dı̅plere  legata p̲ ıp̅a ſn͛ retardac̅one dıﬅrıbu͛e ꝙ dıu re ſue d hec ſe extendu̅t / ꝓuıſo ꝙ ſı u̅ꝙ co̅tıngat Relígıoſa  nob̅ ín xͦ dılc̅a . bb̅tıſſa̅ / ſoꝛoꝛe̅ Cﬅına̅ attædot͛ ſeu lı ſoꝛoꝛe vl̅ co̅uentu̅ ſoꝛoꝝ ſc̅e clare ſu͛ fr̅e mı̅oꝛe Roſꝃ ı̅petı ſeu moleﬅar . occaſíone depo̅ıt ſeu reru̅ depo̅ıtaru̅ p ıp̅a p̲ ſoꝛoꝛe̅ nr̅a ſupdc̅a . que  q pn̅tıbꝫ ꝓteﬅam no ı̅teglıt͛  ſalua ʀecepıſſe  leuaſſe / exnu̅c ꝓ tu̅c  ı̅ om̅e te̅pꝰ oblıgamꝰ no ı̅ íudıcio ecc̅aﬅico vl̅ mu̅dano ſeu coꝛ̅ qbuſỽí lí íudıcıbꝫ ỽl̅ no¦bılıbꝫ ſeu ᷎ ex íudıcíu̅ ꝓ dc̅ı ſoꝛoꝛıbꝫ ſeu co̅uentu oꝛdı̅ ſc̅e clare / c ᷎ ꝓ fr̅ıbꝫ mı̅oꝛıbꝫ qͦꝝ de fc̅o ı̅teﬅ ſeu ı̅te̅e pot͛ıt / om̅ıbꝫ mo rn̅d͛e  ıp̅o ſ p̅míſ ı̅de̅pne co̅ſeruare bſqꝫ om̅ ꝯtdc̅one  ꝓtione on̅ıo ıp̅a ca̅ ſeu ı̅petıc̅oe ı̅ no c mo̅aﬅ͛ıu nr̅ p̅fatu̅ ſſume̅te ꝓmtımꝰ inſup̲ ꝙ clenodı q̅ dc̅a dn̅  ſoꝛoꝛ nr̅a fılıo ſuo petro thøꝛb͛nſ c líí s ttíne̅¦tıbꝫ ſcl̅aríbꝫ legauít . intaa ſerurı debent d rbıtu̅ / dc̅aru̅ ſoꝛoꝝ ſc̅e clare c frͫ mı̅oꝝ ỽſqꝫqͦ ıp̅e petrꝰ p̲ſonalr co̅p̲uerıt vl̅ nu̅cıu̅ fıdedıgnu̅ tnſmıſ͛ıt q dc̅a ſoꝛoꝛe poſſıt qttare de p̅dc̅ı . In Cuıꝰ ʀeí teﬅı̅oı ſıgıll nr̅a / ỽn cu̅ ſıgıll̅ ỽen͛abılıu̅ dn̅oꝝ Dn̅ı Iacobı pauel decanj . Dn̅ı Nıchol̅ duuæ  dn̅í lambert cano̅ıcoꝝ eccl̅ıe Roſkıld̅ c  fr̅ı Ia¦cobı píu poꝛí fr̅ p̅dícatoꝝ ıde̅ pn̅tıbꝫ ſt ae̅ſ . Datu̅ a̅no dn̅ NUMBER_TOKEN NUMBER_TOKEN NUMBER_TOKEN t͛cío In díe ỽndecı̅ mıll̅ ỽgínu̅\n",
      "\n",
      "Loading velux_dipl_danish.npz... done (0.0s)\n",
      "Alle me(n ) th(et)te b(re)ff sse eller hør(e ) helse wy gerickæ hanss(øn ) byfogh(et ) i slauelse pædh(e)r jenss(øn ) och clæmen griis b(ur)gemeste(r ) hans pædh(e)rss(øn ) jes kogge raadme(n ) g(er)loff he(n)rickess(øn ) och pædh(e)r kældss(øn ) byme(m ) i sa(m)me st(et ) k(er)lighe m(et ) gud ku(n)gør(e ) wy alle nær(værende ) och ko(m)mesku(lende ) ath aar efft(er ) gutz byrd NUMBER_TOKEN th(e)n mandagh næst efft(er ) dysmøsse wor(e ) wy nær(værende ) m(et ) manghe fler(e ) gothe me(n ) paa wort bytingh i slauelse hørdhe skællighe och soghe ath beskeden man clæmen pædh(e)rss(øn ) b(ur)ger(e ) i slauelse stodh jnnæn fyræ tingstockæ skøttæ och wplodh och tiil ewindheligh eyæ solde och aff hende paa hedh(e)rligh och welb(ir)digh q(ui)nnæs wegne husf(rv)æ mærde aff gødh(e)rsløff hedh(e)rligh man och renliffueligh h(er ) he(m)mig jeopss(øn ) p(ri)ær(e ) j soræ paa for(nefnde ) closters wegne all th(e)n rættigheed och eyædom som he(n)ne lodne eller tiil falle kw(n)næ j th(e)n gord som andh(e)rs jeopss(øn ) ottæ och j bodhe liggend(e ) i slauelse norden och østerst paa stenstwgade hwilken for(nefnde ) husf(rv)æ mærde haffde giffuet fo{r}(nefnde ) clæmen full macht j forskreffne styckæ offu(er)wærind(e ) fogh(et ) b(ur)gemester(e ) och manghe fle(re ) gothe men j for(nefnde ) clæme(n)s stwæ och kænd(e ) for(nefnde ) clæme(n ) pædh(e)rss(øn ) ath for(nefnde ) husf(rv ) mærde haffde fult och alt wpboriit for(e ) for(nefnde ) eyædom och rættigheed tiil gothe rodhe efft(er ) synæ nøghe Th(e)r offu(er ) tiilbant for(nefnde ) clæm{e(n ) } pædh(e)rss(øn ) segh tiil paa for(nefnde ) husf(rv)æ mærd(es ) wegne och he(n)nes arwin¦ghe for(nefnde ) h(er ) he(m)mig(e ) paa for(nefnde ) clost(eris ) wegne th(e)n forskreffne rættigheed och eyædom frii hemlæ och tiilstonde for hwær mantz gensielse eller tiiltale At swo giik och foor for oss och manghe fler(e ) gothe me(n ) th(et ) witne wy fremdel(es ) m(et ) wor(e ) jnciglæ hengde nædh(e)n for(e ) th(et)te b(re)ff Giffuet aar dagh och steed som for(e ) ær skreffuet\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "for data_source_filename in data_source_filenames:\n",
    "    dataset = load_dataset(data_source_filename)\n",
    "    \n",
    "    instance = random.choice(dataset[\"data\"])\n",
    "    try:\n",
    "        print(instance[\"text\"])\n",
    "    except KeyError:\n",
    "        print(\" \".join(sentence) for sentence in instance[\"tokens\"])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate folds\n",
    "\n",
    "This method is deterministic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading StClare_facs_danish.npz... done (0.0s)\n",
      " Saving StClare_facs_danish.npz... done (0.1s)\n",
      "Loading SDHK_Latin.npz... done (0.8s)\n",
      " Saving SDHK_Latin.npz... done (2.5s)\n",
      "Loading StClare_facs_latin.npz... done (0.0s)\n",
      " Saving StClare_facs_latin.npz... done (0.1s)\n",
      "Loading StClare_dipl_danish.npz... done (0.0s)\n",
      " Saving StClare_dipl_danish.npz... done (0.0s)\n",
      "Loading StClare_dipl_latin.npz... done (0.0s)\n",
      " Saving StClare_dipl_latin.npz... done (0.1s)\n",
      "Loading velux_facs_danish.npz... done (0.0s)\n",
      " Saving velux_facs_danish.npz... done (0.1s)\n",
      "Loading SDHK_Swedish.npz... done (0.3s)\n",
      " Saving SDHK_Swedish.npz... done (0.9s)\n",
      "Loading Colonia.npz... done (5.5s)\n",
      " Saving Colonia.npz... done (11.9s)\n",
      "Loading SemEval2015.npz... done (0.1s)\n",
      " Saving SemEval2015.npz... done (0.3s)\n",
      "Loading velux_dipl_latin.npz... done (0.1s)\n",
      " Saving velux_dipl_latin.npz... done (0.3s)\n",
      "Loading velux_facs_latin.npz... done (0.1s)\n",
      " Saving velux_facs_latin.npz... done (0.4s)\n",
      "Loading velux_dipl_danish.npz... done (0.0s)\n",
      " Saving velux_dipl_danish.npz... done (0.1s)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for data_source_filename in data_source_filenames:\n",
    "    dataset = load_dataset(data_source_filename)\n",
    "    # Get years (average if span)\n",
    "    years = np.asarray([d['date'] if type(d['date']) is int else int((d['date'][0]+d['date'][1])//2) for d in dataset['data']])\n",
    "    # Generate indices\n",
    "    indices = np.arange(len(years))\n",
    "    # Sort by year\n",
    "    new_order = np.argsort(years)\n",
    "    years = years[new_order]\n",
    "    indices = list(indices[new_order])\n",
    "    # Distribute over folds\n",
    "    folds = {'train': list(), 'val': list(), 'test': list()}\n",
    "    keys = ['train', 'train', 'train', 'val', 'test']\n",
    "    i = 0\n",
    "    while len(indices) > 0:\n",
    "        folds[keys[i]].append(indices[0])\n",
    "        i += 1\n",
    "        i = i % len(keys)\n",
    "        indices = indices[1:]\n",
    "    # Commit to data structure\n",
    "    dataset['folds'] = folds\n",
    "    # Verify size\n",
    "    assert len(dataset['folds']['train'])+len(dataset['folds']['val'])+len(dataset['folds']['test']) == len(years)\n",
    "    # Verify uniqueness\n",
    "    all_folds = list()\n",
    "    all_folds.extend(dataset['folds']['train'])\n",
    "    all_folds.extend(dataset['folds']['val'])\n",
    "    all_folds.extend(dataset['folds']['test'])\n",
    "    assert len(set(all_folds)) == len(years)\n",
    "    # Save\n",
    "    print(\" Saving %s... \" % data_source_filename.split(\"/\")[-1], end=\"\")\n",
    "    t = time.time()\n",
    "    np.savez_compressed(data_source_filename, **dataset)\n",
    "    print(\"done (%.1fs)\" % (time.time()-t), flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UJNfVSisUhmz"
   },
   "source": [
    "# Feature extraction\n",
    "\n",
    "* Word, POS, and Word+POS n-grams\n",
    "  * tf-idf vectors\n",
    "  * BOW vectors\n",
    "* Character n-grams\n",
    "  * Frequency vectors\n",
    "  * BOW vectors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3_DYYLuj23av"
   },
   "source": [
    "##  Zampieri style word and pos vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 357
    },
    "colab_type": "code",
    "id": "ihUZPgap02il",
    "outputId": "4e030a2a-8f61-46c8-92db-56ca261f7fac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading StClare_facs_danish.npz... done (0.0s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " Preparing word(+pos) data: 100%|██████████| 98/98 [00:00<00:00, 21672.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Running BOW feature transform... done (0.0s)\n",
      " Running tf-idf feature transform... done (0.0s)\n",
      " Saving StClare_facs_danish.npz... done (0.0s)\n",
      "Loading SDHK_Latin.npz... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done (0.8s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " Preparing word(+pos) data: 100%|██████████| 7572/7572 [00:00<00:00, 42575.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Running BOW feature transform... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done (2.8s)\n",
      " Running tf-idf feature transform... done (2.9s)\n",
      " Saving SDHK_Latin.npz... done (3.4s)\n",
      "Loading StClare_facs_latin.npz... done (0.0s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " Preparing word(+pos) data: 100%|██████████| 358/358 [00:00<00:00, 29744.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Running BOW feature transform... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done (0.1s)\n",
      " Running tf-idf feature transform... done (0.1s)\n",
      " Saving StClare_facs_latin.npz... done (0.1s)\n",
      "Loading StClare_dipl_danish.npz... done (0.0s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " Preparing word(+pos) data: 100%|██████████| 98/98 [00:00<00:00, 8556.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Running BOW feature transform... done (0.0s)\n",
      " Running tf-idf feature transform... done (0.0s)\n",
      " Saving StClare_dipl_danish.npz... done (0.0s)\n",
      "Loading StClare_dipl_latin.npz... done (0.0s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " Preparing word(+pos) data: 100%|██████████| 358/358 [00:00<00:00, 43142.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Running BOW feature transform... done (0.1s)\n",
      " Running tf-idf feature transform... done (0.1s)\n",
      " Saving StClare_dipl_latin.npz... done (0.1s)\n",
      "Loading velux_facs_danish.npz... done (0.0s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " Preparing word(+pos) data: 100%|██████████| 98/98 [00:00<00:00, 21492.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Running BOW feature transform... done (0.0s)\n",
      " Running tf-idf feature transform... done (0.0s)\n",
      " Saving velux_facs_danish.npz... done (0.1s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading SDHK_Swedish.npz... done (0.3s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " Preparing word(+pos) data: 100%|██████████| 3086/3086 [00:00<00:00, 39432.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Running BOW feature transform... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done (1.0s)\n",
      " Running tf-idf feature transform... done (1.0s)\n",
      " Saving SDHK_Swedish.npz... done (1.3s)\n",
      "Loading Colonia.npz... done (5.6s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " Preparing word(+pos) data: 100%|██████████| 99/99 [00:01<00:00, 72.70it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Running BOW feature transform... done (14.7s)\n",
      " Running tf-idf feature transform... done (15.0s)\n",
      " Saving Colonia.npz... done (13.4s)\n",
      "Loading SemEval2015.npz... done (0.1s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " Preparing word(+pos) data: 100%|██████████| 3370/3370 [00:00<00:00, 86735.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Running BOW feature transform... done (0.3s)\n",
      " Running tf-idf feature transform... done (0.3s)\n",
      " Saving SemEval2015.npz... done (0.4s)\n",
      "Loading velux_dipl_latin.npz... done (0.1s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " Preparing word(+pos) data: 100%|██████████| 358/358 [00:00<00:00, 28923.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Running BOW feature transform... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done (0.1s)\n",
      " Running tf-idf feature transform... done (0.1s)\n",
      " Saving velux_dipl_latin.npz... done (0.3s)\n",
      "Loading velux_facs_latin.npz... done (0.1s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " Preparing word(+pos) data: 100%|██████████| 358/358 [00:00<00:00, 29136.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Running BOW feature transform... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done (0.1s)\n",
      " Running tf-idf feature transform... done (0.1s)\n",
      " Saving velux_facs_latin.npz... done (0.4s)\n",
      "Loading velux_dipl_danish.npz... done (0.0s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " Preparing word(+pos) data: 100%|██████████| 98/98 [00:00<00:00, 22024.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Running BOW feature transform... done (0.0s)\n",
      " Running tf-idf feature transform... done (0.0s)\n",
      " Saving velux_dipl_danish.npz... done (0.1s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "for data_source_filename in data_source_filenames:\n",
    "    # Load data set\n",
    "    dataset = load_dataset(data_source_filename)\n",
    "\n",
    "    # Get training and validation sets for training the tf-idf\n",
    "    train_val_set = list()\n",
    "    train_val_set.extend(dataset['folds']['train'])\n",
    "    train_val_set.extend(dataset['folds']['val'])\n",
    "    train_val_set = np.asarray(train_val_set, dtype=np.int)\n",
    "\n",
    "    # Collect data\n",
    "    X_word_documents = list()\n",
    "    X_pos_documents = list()\n",
    "    X_word_pos_documents = list()\n",
    "    if 'tokens' in dataset['data'][0]:\n",
    "        for item in tqdm(dataset['data'], desc=\" Preparing word(+pos) data\"):\n",
    "            # Create \"token\" and \"pos\" document for the count vectorizer below\n",
    "            a = [word.lower() for sent in item['tokens'] for word in sent]\n",
    "            X_word_documents.append(\" \".join(a))\n",
    "            if 'pos' in item:\n",
    "                b = [word for sent in item['pos'] for word in sent]\n",
    "                assert len(a)==len(b)\n",
    "                X_pos_documents.append(\" \".join(b))\n",
    "                X_word_pos_documents.append(\" \".join([x+y for x, y in zip(a, b)]))\n",
    "        if 'pos' in dataset['data'][0]:\n",
    "            assert len(X_word_documents)==len(X_pos_documents)\n",
    "    else:\n",
    "        print(\" Nothing to process\")\n",
    "\n",
    "    # Add feature set dict\n",
    "    if 'feature_sets' not in dataset:\n",
    "      dataset['feature_sets'] = dict()\n",
    "\n",
    "    if len(X_word_documents)>0:\n",
    "        print(\" Running BOW feature transform... \", end=\"\")\n",
    "        t = time.time()\n",
    "        vectoriser = CountVectorizer().fit(np.asarray(X_word_documents)[train_val_set])\n",
    "        X = vectoriser.transform(X_word_documents)\n",
    "        X[X>1] = 1\n",
    "        dataset['feature_sets']['bow_words'] = X\n",
    "        if len(X_pos_documents)>0:\n",
    "            vectoriser = CountVectorizer().fit(np.asarray(X_pos_documents)[train_val_set])\n",
    "            X = vectoriser.transform(X_pos_documents)\n",
    "            X[X>1] = 1\n",
    "            dataset['feature_sets']['bow_pos'] = X\n",
    "            vectoriser = CountVectorizer().fit(np.asarray(X_word_pos_documents)[train_val_set])\n",
    "            X = vectoriser.transform(X_word_pos_documents)\n",
    "            X[X>1] = 1\n",
    "            dataset['feature_sets']['bow_words_pos'] = X\n",
    "        print(\"done (%.1fs)\" % (time.time()-t))\n",
    "\n",
    "        print(\" Running tf-idf feature transform... \", end=\"\")\n",
    "        t = time.time()\n",
    "        vectoriser = TfidfVectorizer().fit(np.asarray(X_word_documents)[train_val_set])\n",
    "        X = vectoriser.transform(X_word_documents)\n",
    "        #X[X>1] = 1\n",
    "        dataset['feature_sets']['tfidf_words'] = X\n",
    "        if len(X_pos_documents)>0:\n",
    "            vectoriser = TfidfVectorizer().fit(np.asarray(X_pos_documents)[train_val_set])\n",
    "            X = vectoriser.transform(X_pos_documents)\n",
    "            #X[X>1] = 1\n",
    "            dataset['feature_sets']['tfidf_pos'] = X\n",
    "            vectoriser = TfidfVectorizer().fit(np.asarray(X_word_pos_documents)[train_val_set])\n",
    "            X = vectoriser.transform(X_word_pos_documents)\n",
    "            #X[X>1] = 1\n",
    "            dataset['feature_sets']['tfidf_words_pos'] = X\n",
    "        print(\"done (%.1fs)\" % (time.time()-t))\n",
    "\n",
    "    print(\" Saving %s... \" % data_source_filename.split(\"/\")[-1], end=\"\")\n",
    "    t = time.time()\n",
    "    np.savez_compressed(data_source_filename, **dataset)\n",
    "    print(\"done (%.1fs)\" % (time.time()-t), flush=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ipiF9sjE2uUw"
   },
   "source": [
    "## n-gram vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "colab_type": "code",
    "id": "P1SbNGASktUM",
    "outputId": "451c1d2e-38a7-4f20-950f-0bcae4f636b2"
   },
   "outputs": [],
   "source": [
    "urlretrieve(\"https://raw.githubusercontent.com/fredrikwahlberg/5LN721/master/ngram.py\", \n",
    "            \"external/ngram.py\")\n",
    "\n",
    "from ngram import NGramModel\n",
    "from scipy.sparse import lil_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 697
    },
    "colab_type": "code",
    "id": "ZexMostSSDeG",
    "outputId": "b00a03be-6c71-40b3-9fbc-a6ae35709206"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading StClare_facs_danish.npz... done (0.0s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " Creating ngram models: 100%|██████████| 98/98 [00:00<00:00, 850.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Creating reference models...done (0.1s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " Building matrix for character_ngram_1: 100%|██████████| 98/98 [00:00<00:00, 8630.44it/s]\n",
      " Building matrix for character_ngram_2: 100%|██████████| 98/98 [00:00<00:00, 4425.23it/s]\n",
      " Building matrix for character_ngram_3: 100%|██████████| 98/98 [00:00<00:00, 1957.03it/s]\n",
      " Building matrix for word_ngram_1: 100%|██████████| 98/98 [00:00<00:00, 4482.51it/s]\n",
      " Building matrix for word_ngram_2: 100%|██████████| 98/98 [00:00<00:00, 2694.38it/s]\n",
      " Building matrix for word_ngram_3: 100%|██████████| 98/98 [00:00<00:00, 2379.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving StClare_facs_danish.npz... done (0.1s)\n",
      "Loading SDHK_Latin.npz... done (1.0s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " Creating ngram models: 100%|██████████| 7572/7572 [00:04<00:00, 1774.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Creating reference models...done (4.4s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " Building matrix for character_ngram_1: 100%|██████████| 7572/7572 [00:00<00:00, 14092.55it/s]\n",
      " Building matrix for character_ngram_2: 100%|██████████| 7572/7572 [00:01<00:00, 6920.26it/s]\n",
      " Building matrix for character_ngram_3: 100%|██████████| 7572/7572 [00:03<00:00, 2218.07it/s]\n",
      " Building matrix for word_ngram_1: 100%|██████████| 7572/7572 [00:06<00:00, 1199.91it/s]\n",
      " Building matrix for word_ngram_2: 100%|██████████| 7572/7572 [00:35<00:00, 210.76it/s]\n",
      " Building matrix for word_ngram_3: 100%|██████████| 7572/7572 [01:02<00:00, 120.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving SDHK_Latin.npz... done (6.8s)\n",
      "Loading StClare_facs_latin.npz... done (0.0s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " Creating ngram models: 100%|██████████| 358/358 [00:00<00:00, 1067.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Creating reference models...done (0.6s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " Building matrix for character_ngram_1: 100%|██████████| 358/358 [00:00<00:00, 11206.60it/s]\n",
      " Building matrix for character_ngram_2: 100%|██████████| 358/358 [00:00<00:00, 4532.59it/s]\n",
      " Building matrix for character_ngram_3: 100%|██████████| 358/358 [00:00<00:00, 1635.49it/s]\n",
      " Building matrix for word_ngram_1: 100%|██████████| 358/358 [00:00<00:00, 3149.31it/s]\n",
      " Building matrix for word_ngram_2: 100%|██████████| 358/358 [00:00<00:00, 1719.77it/s]\n",
      " Building matrix for word_ngram_3: 100%|██████████| 358/358 [00:00<00:00, 1503.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving StClare_facs_latin.npz... done (0.4s)\n",
      "Loading StClare_dipl_danish.npz... done (0.0s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " Creating ngram models: 100%|██████████| 98/98 [00:00<00:00, 772.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Creating reference models...done (0.1s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " Building matrix for character_ngram_1: 100%|██████████| 98/98 [00:00<00:00, 11235.87it/s]\n",
      " Building matrix for character_ngram_2: 100%|██████████| 98/98 [00:00<00:00, 5357.13it/s]\n",
      " Building matrix for character_ngram_3: 100%|██████████| 98/98 [00:00<00:00, 2571.45it/s]\n",
      " Building matrix for word_ngram_1: 100%|██████████| 98/98 [00:00<00:00, 4516.25it/s]\n",
      " Building matrix for word_ngram_2: 100%|██████████| 98/98 [00:00<00:00, 2670.98it/s]\n",
      " Building matrix for word_ngram_3: 100%|██████████| 98/98 [00:00<00:00, 2374.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving StClare_dipl_danish.npz... done (0.1s)\n",
      "Loading StClare_dipl_latin.npz... done (0.0s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " Creating ngram models: 100%|██████████| 358/358 [00:00<00:00, 1482.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Creating reference models...done (0.2s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " Building matrix for character_ngram_1: 100%|██████████| 358/358 [00:00<00:00, 13631.35it/s]\n",
      " Building matrix for character_ngram_2: 100%|██████████| 358/358 [00:00<00:00, 6796.39it/s]\n",
      " Building matrix for character_ngram_3: 100%|██████████| 358/358 [00:00<00:00, 2751.68it/s]\n",
      " Building matrix for word_ngram_1: 100%|██████████| 358/358 [00:00<00:00, 4690.79it/s]\n",
      " Building matrix for word_ngram_2: 100%|██████████| 358/358 [00:00<00:00, 2147.40it/s]\n",
      " Building matrix for word_ngram_3: 100%|██████████| 358/358 [00:00<00:00, 1738.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving StClare_dipl_latin.npz... done (0.3s)\n",
      "Loading velux_facs_danish.npz... done (0.0s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " Creating ngram models: 100%|██████████| 98/98 [00:00<00:00, 636.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Creating reference models...done (0.1s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " Building matrix for character_ngram_1: 100%|██████████| 98/98 [00:00<00:00, 10488.44it/s]\n",
      " Building matrix for character_ngram_2: 100%|██████████| 98/98 [00:00<00:00, 4439.09it/s]\n",
      " Building matrix for character_ngram_3: 100%|██████████| 98/98 [00:00<00:00, 1827.15it/s]\n",
      " Building matrix for word_ngram_1: 100%|██████████| 98/98 [00:00<00:00, 4330.08it/s]\n",
      " Building matrix for word_ngram_2: 100%|██████████| 98/98 [00:00<00:00, 2554.93it/s]\n",
      " Building matrix for word_ngram_3: 100%|██████████| 98/98 [00:00<00:00, 2266.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving velux_facs_danish.npz... done (0.1s)\n",
      "Loading SDHK_Swedish.npz... done (0.4s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " Creating ngram models: 100%|██████████| 3086/3086 [00:01<00:00, 1743.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Creating reference models...done (1.9s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " Building matrix for character_ngram_1: 100%|██████████| 3086/3086 [00:00<00:00, 13518.23it/s]\n",
      " Building matrix for character_ngram_2: 100%|██████████| 3086/3086 [00:00<00:00, 6211.61it/s]\n",
      " Building matrix for character_ngram_3: 100%|██████████| 3086/3086 [00:01<00:00, 2151.39it/s]\n",
      " Building matrix for word_ngram_1: 100%|██████████| 3086/3086 [00:01<00:00, 1551.47it/s]\n",
      " Building matrix for word_ngram_2: 100%|██████████| 3086/3086 [00:07<00:00, 425.13it/s]\n",
      " Building matrix for word_ngram_3: 100%|██████████| 3086/3086 [00:11<00:00, 275.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving SDHK_Swedish.npz... done (2.8s)\n",
      "Loading Colonia.npz... done (5.7s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " Creating ngram models: 100%|██████████| 99/99 [00:14<00:00,  6.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Creating reference models...done (6.1s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " Building matrix for character_ngram_1: 100%|██████████| 99/99 [00:00<00:00, 11110.89it/s]\n",
      " Building matrix for character_ngram_2: 100%|██████████| 99/99 [00:00<00:00, 3115.94it/s]\n",
      " Building matrix for character_ngram_3: 100%|██████████| 99/99 [00:00<00:00, 519.51it/s]\n",
      " Building matrix for word_ngram_1: 100%|██████████| 99/99 [00:00<00:00, 154.97it/s]\n",
      " Building matrix for word_ngram_2: 100%|██████████| 99/99 [00:04<00:00, 22.89it/s]\n",
      " Building matrix for word_ngram_3: 100%|██████████| 99/99 [00:07<00:00, 13.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving Colonia.npz... done (16.3s)\n",
      "Loading SemEval2015.npz... done (0.3s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " Creating ngram models: 100%|██████████| 3370/3370 [00:01<00:00, 3129.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Creating reference models...done (1.3s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " Building matrix for character_ngram_1: 100%|██████████| 3370/3370 [00:00<00:00, 13949.62it/s]\n",
      " Building matrix for character_ngram_2: 100%|██████████| 3370/3370 [00:00<00:00, 8080.04it/s]\n",
      " Building matrix for character_ngram_3: 100%|██████████| 3370/3370 [00:00<00:00, 4001.09it/s]\n",
      " Building matrix for word_ngram_1: 100%|██████████| 3370/3370 [00:00<00:00, 4188.22it/s]\n",
      " Building matrix for word_ngram_2: 100%|██████████| 3370/3370 [00:03<00:00, 1089.72it/s]\n",
      " Building matrix for word_ngram_3: 100%|██████████| 3370/3370 [00:04<00:00, 714.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving SemEval2015.npz... done (1.1s)\n",
      "Loading velux_dipl_latin.npz... done (0.1s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " Creating ngram models: 100%|██████████| 358/358 [00:00<00:00, 1162.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Creating reference models...done (0.4s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " Building matrix for character_ngram_1: 100%|██████████| 358/358 [00:00<00:00, 13453.76it/s]\n",
      " Building matrix for character_ngram_2: 100%|██████████| 358/358 [00:00<00:00, 6337.89it/s]\n",
      " Building matrix for character_ngram_3: 100%|██████████| 358/358 [00:00<00:00, 2330.63it/s]\n",
      " Building matrix for word_ngram_1: 100%|██████████| 358/358 [00:00<00:00, 4080.21it/s]\n",
      " Building matrix for word_ngram_2: 100%|██████████| 358/358 [00:00<00:00, 1916.44it/s]\n",
      " Building matrix for word_ngram_3: 100%|██████████| 358/358 [00:00<00:00, 1441.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving velux_dipl_latin.npz... done (0.3s)\n",
      "Loading velux_facs_latin.npz... done (0.1s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " Creating ngram models: 100%|██████████| 358/358 [00:00<00:00, 1213.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Creating reference models...done (0.3s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " Building matrix for character_ngram_1: 100%|██████████| 358/358 [00:00<00:00, 8059.73it/s]\n",
      " Building matrix for character_ngram_2: 100%|██████████| 358/358 [00:00<00:00, 3587.16it/s]\n",
      " Building matrix for character_ngram_3: 100%|██████████| 358/358 [00:00<00:00, 1581.44it/s]\n",
      " Building matrix for word_ngram_1: 100%|██████████| 358/358 [00:00<00:00, 3074.80it/s]\n",
      " Building matrix for word_ngram_2: 100%|██████████| 358/358 [00:00<00:00, 1675.04it/s]\n",
      " Building matrix for word_ngram_3: 100%|██████████| 358/358 [00:00<00:00, 1502.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving velux_facs_latin.npz... done (0.4s)\n",
      "Loading velux_dipl_danish.npz... done (0.0s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " Creating ngram models: 100%|██████████| 98/98 [00:00<00:00, 745.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Creating reference models...done (0.1s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " Building matrix for character_ngram_1: 100%|██████████| 98/98 [00:00<00:00, 12310.33it/s]\n",
      " Building matrix for character_ngram_2: 100%|██████████| 98/98 [00:00<00:00, 5273.35it/s]\n",
      " Building matrix for character_ngram_3: 100%|██████████| 98/98 [00:00<00:00, 2342.12it/s]\n",
      " Building matrix for word_ngram_1: 100%|██████████| 98/98 [00:00<00:00, 4697.35it/s]\n",
      " Building matrix for word_ngram_2: 100%|██████████| 98/98 [00:00<00:00, 2666.16it/s]\n",
      " Building matrix for word_ngram_3: 100%|██████████| 98/98 [00:00<00:00, 2273.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving velux_dipl_danish.npz... done (0.1s)\n"
     ]
    }
   ],
   "source": [
    "for data_source_filename in data_source_filenames:\n",
    "    # Load the data (and maybe add a key)\n",
    "    dataset = load_dataset(data_source_filename)\n",
    "    if 'feature_sets' not in dataset:\n",
    "        dataset['feature_sets'] = dict()\n",
    "\n",
    "    # Define an ngram extraction function for processing documents\n",
    "    # characters = list(dataset['data'][0]['text'])\n",
    "    # tokens = [word.lower() for sent in item['tokens'] for word in sent]\n",
    "    def _make_ngrams(item):\n",
    "        from ngram import NGramModel\n",
    "        # Data word word ngrams\n",
    "        if 'tokens' in item.keys():\n",
    "            # Flatten and lower() the sentences\n",
    "            tokens = [word.lower() for sent in item['tokens'] for word in sent]\n",
    "        else:\n",
    "            # Improvise from raw text\n",
    "            tokens = item['text'].lower().split()\n",
    "        # Data for character ngrams\n",
    "        if 'text' in item:\n",
    "            characters = list(item['text'].lower())\n",
    "        else:\n",
    "            characters = list(\" \".join(tokens).lower())\n",
    "        # Make the models\n",
    "        ret = dict()\n",
    "        for n_order in range(1, 3+1):\n",
    "            ret[\"word_ngram_%i\" % n_order] = NGramModel(tokens, order=n_order)\n",
    "            ret[\"character_ngram_%i\" % n_order] = NGramModel(characters, order=n_order)\n",
    "        return ret\n",
    "\n",
    "    # Create models for all documents and model configurations\n",
    "    ngram_models = dict()\n",
    "    with Pool(processes=os.cpu_count()) as pool:\n",
    "        for models in tqdm(pool.imap(_make_ngrams, dataset['data'], chunksize=10), desc=\" Creating ngram models\", total=len(dataset['data'])):\n",
    "            for k in models.keys():\n",
    "                if k not in ngram_models:\n",
    "                    ngram_models[k] = list()\n",
    "                ngram_models[k].append(models[k])\n",
    "    # Verify that all the models are there\n",
    "    for k in ngram_models.keys():\n",
    "        assert len(ngram_models[k]) == len(dataset['data'])\n",
    "\n",
    "    # Create the reference models\n",
    "    print(\" Creating reference models...\", end=\"\")\n",
    "    t = time.time()\n",
    "    ngram_reference_model = dict()\n",
    "    for k in ngram_models.keys():\n",
    "        for i, model in enumerate(ngram_models[k]):\n",
    "            if i==0:\n",
    "                ngram_reference_model[k] = model.copy()\n",
    "            else:\n",
    "                ngram_reference_model[k] = ngram_reference_model[k].union_update(model)\n",
    "    print(\"done (%.1fs)\" % (time.time()-t), flush=True)\n",
    "\n",
    "    # Vectorize and make sparse feature matrices\n",
    "    keys = list(ngram_reference_model.keys())\n",
    "    keys.sort()\n",
    "    for k in keys:\n",
    "        X = lil_matrix((len(dataset['data']), len(ngram_reference_model[k])), dtype=np.float)\n",
    "        for i, model in tqdm(enumerate(ngram_models[k]), desc=\" Building matrix for %s\" % k, total=len(dataset['data'])):\n",
    "            v = model.vectorize(codebook=ngram_reference_model[k].codebook())\n",
    "            for j in np.nonzero(v):\n",
    "                X[i, j] = v[j]\n",
    "        dataset['feature_sets'][k] = X.tocsr()\n",
    "\n",
    "    print(\"Saving %s... \" % data_source_filename.split(\"/\")[-1], end=\"\")\n",
    "    t = time.time()\n",
    "    np.savez_compressed(data_source_filename, **dataset)\n",
    "    print(\"done (%.1fs)\" % (time.time()-t), flush=True)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Dating - Setting up data sets.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
