{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "E53gPuzAh2OV"
   },
   "source": [
    "# Gaussian process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "exLjKh6Rv2rI"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import os.path\n",
    "import numpy as np\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "assert os.path.exists('data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "SGJirJ09zZet",
    "outputId": "0350b466-b606-4529-a902-52c738793c15"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data/StClare_facs_danish.npz',\n",
       " 'data/SDHK_Latin.npz',\n",
       " 'data/StClare_facs_latin.npz',\n",
       " 'data/StClare_dipl_danish.npz',\n",
       " 'data/StClare_dipl_latin.npz',\n",
       " 'data/SDHK_Swedish.npz',\n",
       " 'data/Colonia.npz',\n",
       " 'data/SemEval2015.npz']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def load_dataset(data_source_filename):\n",
    "  print(\"Loading %s... \" % data_source_filename.split(\"/\")[-1], end=\"\")\n",
    "  t = time.time()\n",
    "  dataset = dict()\n",
    "  with np.load(data_source_filename, allow_pickle=True) as source_file:\n",
    "    for key in source_file.keys():\n",
    "      # print(key)\n",
    "      dataset[key] = source_file[key].tolist()\n",
    "  print(\"done (%.1fs)\" % (time.time()-t), flush=True)\n",
    "  return dataset\n",
    "\n",
    "data_source_filenames = [os.path.join('data', fn) for fn in os.listdir('data')\n",
    "                            if os.path.isfile(os.path.join('data', fn)) and fn[-3:]=='npz']\n",
    "data_source_filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cfyNm3q7WBb0"
   },
   "outputs": [],
   "source": [
    "#from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "#from sklearn.gaussian_process.kernels import RBF, WhiteKernel, ConstantKernel\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.metrics import accuracy_score\n",
    "import GPy\n",
    "\n",
    "bin_width = 25\n",
    "n_samples_limit = 3000\n",
    "n_features_ard_limit = 100\n",
    "n_features_max = 1000\n",
    "\n",
    "results = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading SDHK_Latin.npz... done (1.6s)\n",
      " Training data too large (6058), subsampling to 3000\n",
      " Running estimations\n",
      "  Loaded feature set bow_words\n",
      "   Reducing feature space\n",
      "   No ARD kernel (1000 features)\n",
      "  Making the estimator\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " /home/fredrik/anaconda3/lib/python3.7/site-packages/GPy/kern/src/stationary.py:168: RuntimeWarning:overflow encountered in true_divide\n",
      " /home/fredrik/anaconda3/lib/python3.7/site-packages/GPy/kern/src/rbf.py:52: RuntimeWarning:overflow encountered in square\n",
      " /home/fredrik/anaconda3/lib/python3.7/site-packages/GPy/kern/src/rbf.py:76: RuntimeWarning:invalid value encountered in multiply\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization restart 1/5, f = 16496.784277690036\n",
      "Optimization restart 2/5, f = 15518.492757979144\n",
      "Optimization restart 3/5, f = 15518.492757513788\n",
      "Optimization restart 4/5, f = 18263.95785741519\n",
      "Optimization restart 5/5, f = 16336.184595510487\n",
      "  Loaded feature set tfidf_words\n",
      "   Reducing feature space\n",
      "   No ARD kernel (1000 features)\n",
      "  Making the estimator\n",
      "Optimization restart 1/5, f = 15544.71417137913\n",
      "Optimization restart 2/5, f = 16335.634422844563\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " /home/fredrik/anaconda3/lib/python3.7/site-packages/GPy/kern/src/stationary.py:168: RuntimeWarning:overflow encountered in true_divide\n",
      " /home/fredrik/anaconda3/lib/python3.7/site-packages/GPy/kern/src/rbf.py:52: RuntimeWarning:overflow encountered in square\n",
      " /home/fredrik/anaconda3/lib/python3.7/site-packages/GPy/kern/src/rbf.py:76: RuntimeWarning:invalid value encountered in multiply\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization restart 3/5, f = 22891.841686470267\n",
      "Optimization restart 4/5, f = 16337.125194360862\n",
      "Optimization restart 5/5, f = 16336.38644926733\n",
      "  Loaded feature set character_ngram_1\n",
      "   No ARD kernel (103 features)\n",
      "  Making the estimator\n",
      "Optimization restart 1/5, f = 16335.564325504609\n",
      "Optimization restart 2/5, f = 16337.022830769194\n",
      "Optimization restart 3/5, f = 16335.572772053183\n",
      "Optimization restart 4/5, f = 15399.664429152283\n",
      "Optimization restart 5/5, f = 15399.66442912534\n",
      "  Loaded feature set character_ngram_2\n",
      "   Reducing feature space\n",
      "   No ARD kernel (1000 features)\n",
      "  Making the estimator\n",
      "Optimization restart 1/5, f = 14927.92543260844\n",
      "Optimization restart 2/5, f = 16335.569191441182\n",
      "Optimization restart 3/5, f = 14927.925432729784\n",
      "Optimization restart 4/5, f = 14927.925432596101\n",
      "Optimization restart 5/5, f = 14927.925432685774\n",
      "  Loaded feature set character_ngram_3\n",
      "   Reducing feature space\n",
      "   No ARD kernel (1000 features)\n",
      "  Making the estimator\n",
      "Optimization restart 1/5, f = 14698.17581455343\n",
      "Optimization restart 2/5, f = 14698.175814786431\n",
      "Optimization restart 3/5, f = 14698.175814523482\n",
      "Optimization restart 4/5, f = 14698.175814522769\n",
      "Optimization restart 5/5, f = 16336.585688013398\n",
      "  Loaded feature set word_ngram_1\n",
      "   Reducing feature space\n",
      "   No ARD kernel (1000 features)\n",
      "  Making the estimator\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " /home/fredrik/anaconda3/lib/python3.7/site-packages/GPy/kern/src/rbf.py:52: RuntimeWarning:overflow encountered in square\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization restart 1/5, f = 14915.357935482685\n",
      "Optimization restart 2/5, f = 16335.568518489754\n",
      "Optimization restart 3/5, f = 14915.357935480075\n",
      "Optimization restart 4/5, f = 14915.35793549479\n",
      "Optimization restart 5/5, f = 16335.570929807274\n",
      "  Loaded feature set word_ngram_2\n",
      "   Reducing feature space\n",
      "   No ARD kernel (1000 features)\n",
      "  Making the estimator\n",
      "Optimization restart 1/5, f = 16335.58331051464\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " /home/fredrik/anaconda3/lib/python3.7/site-packages/GPy/kern/src/rbf.py:52: RuntimeWarning:overflow encountered in square\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization restart 2/5, f = 15820.2687651651\n",
      "Optimization restart 3/5, f = 15869.768597371627\n",
      "Optimization restart 4/5, f = 16335.974281600484\n",
      "Optimization restart 5/5, f = 16335.568639522058\n",
      "  Loaded feature set word_ngram_3\n",
      "   Reducing feature space\n",
      "   No ARD kernel (1000 features)\n",
      "  Making the estimator\n",
      "Optimization restart 1/5, f = 16044.102773290568\n",
      "Optimization restart 2/5, f = 16335.583712175936\n",
      "Optimization restart 3/5, f = 16335.687197499046\n",
      "Optimization restart 4/5, f = 16335.777082045812\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " /home/fredrik/anaconda3/lib/python3.7/site-packages/GPy/kern/src/rbf.py:52: RuntimeWarning:overflow encountered in square\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization restart 5/5, f = 16467.170467337728\n",
      "\n",
      " Results\n",
      "  bow_words, 27.0%\n",
      "  unique predicted classes: [1075. 1150. 1175. 1200. 1225. 1250. 1275. 1300. 1325. 1350. 1375. 1400.\n",
      " 1425. 1450. 1475. 1500. 1525. 1550. 1575.]\n",
      "  tfidf_words, 27.5%\n",
      "  unique predicted classes: [1150. 1175. 1200. 1225. 1250. 1275. 1300. 1325. 1350. 1375. 1400. 1425.\n",
      " 1450. 1475. 1500. 1525. 1550.]\n",
      "  character_ngram_1, 34.2%\n",
      "  unique predicted classes: [1075. 1125. 1150. 1175. 1200. 1225. 1250. 1275. 1300. 1325. 1350. 1375.\n",
      " 1400. 1425. 1450. 1475. 1500. 1525. 1600. 1750.]\n",
      "  character_ngram_2, 36.1%\n",
      "  unique predicted classes: [1125. 1150. 1175. 1200. 1225. 1250. 1275. 1300. 1325. 1350. 1375. 1400.\n",
      " 1425. 1450. 1475. 1500. 1525.]\n",
      "  character_ngram_3, 39.6%\n",
      "  unique predicted classes: [1100. 1150. 1175. 1200. 1225. 1250. 1275. 1300. 1325. 1350. 1375. 1400.\n",
      " 1425. 1450. 1475. 1500. 1525.]\n",
      "  word_ngram_1, 42.1%\n",
      "  unique predicted classes: [1175. 1200. 1225. 1250. 1275. 1300. 1325. 1350. 1375. 1400. 1425. 1450.\n",
      " 1475. 1500. 1525. 1550.]\n",
      "  word_ngram_2, 28.7%\n",
      "  unique predicted classes: [1100. 1175. 1200. 1225. 1250. 1275. 1300. 1325. 1350. 1375. 1400. 1425.\n",
      " 1450. 1475. 1500. 1525. 1550. 1575. 1600. 1625.]\n",
      "  word_ngram_3, 26.6%\n",
      "  unique predicted classes: [1125. 1250. 1275. 1300. 1325. 1350. 1375. 1400. 1425. 1450. 1475. 1500.\n",
      " 1525. 1550. 1575. 1600.]\n"
     ]
    }
   ],
   "source": [
    "#for data_source_filename in set(data_source_filenames).difference(set(results.keys())):\n",
    "for data_source_filename in data_source_filenames[1:2]:\n",
    "    dataset = load_dataset(data_source_filename)\n",
    "    y = np.asarray([np.mean(item['date']) if type(item['date']) is tuple else item['date'] for item in dataset['data']], np.int)\n",
    "    #print(\" Classes in data: %s\" % np.unique(y))\n",
    "    \n",
    "    training_set = dataset['folds']['train']\n",
    "    validation_set = dataset['folds']['val']\n",
    "    test_set = dataset['folds']['test']\n",
    "    training_validation_set = list()\n",
    "    training_validation_set.extend(training_set)\n",
    "    training_validation_set.extend(validation_set)\n",
    "    \n",
    "    # Subsample\n",
    "    if len(training_validation_set) > n_samples_limit:\n",
    "        print(\" Training data too large (%i), subsampling to %i\" % (len(training_validation_set), n_samples_limit))\n",
    "        training_validation_set = np.asarray(training_validation_set)[np.linspace(0, len(training_validation_set), n_samples_limit, endpoint=False, dtype=np.int)]\n",
    "\n",
    "    results[data_source_filename] = dict()\n",
    "    print(\" Running estimations\", flush=True)\n",
    "    for feature_set in list(dataset['feature_sets'].keys()):\n",
    "        X = dataset['feature_sets'][feature_set]\n",
    "        print(\"  Loaded feature set %s\" % feature_set)\n",
    "        n_samples, n_features = X.shape\n",
    "        y_pred = np.zeros(y.shape)\n",
    "\n",
    "        # Chi^2 feature selection\n",
    "        if n_features > n_features_max:\n",
    "            print(\"   Reducing feature space\")\n",
    "            given_data = list()\n",
    "            given_data.extend(training_set)\n",
    "            given_data.extend(validation_set)\n",
    "            feature_selector = SelectKBest(chi2, k=n_features_max).fit(X[given_data], y[given_data])\n",
    "            X = feature_selector.transform(X)\n",
    "            n_features = X.shape[1]\n",
    "\n",
    "        if n_features < n_features_ard_limit:\n",
    "            print(\"   ARD kernel (%i features)\" % (n_features))\n",
    "            kernel = GPy.kern.RBF(input_dim=n_features, ARD=True)\n",
    "            #kernel = ConstantKernel()*RBF(length_scale=np.ones(n_features)) + WhiteKernel()\n",
    "        else:\n",
    "            print(\"   No ARD kernel (%i features)\" % (n_features))\n",
    "            kernel = GPy.kern.RBF(input_dim=n_features)\n",
    "            #kernel = ConstantKernel()*RBF() + WhiteKernel()\n",
    "\n",
    "        #estimator = GaussianProcessRegressor(kernel=kernel, n_restarts_optimizer=10, normalize_y=True)\n",
    "        #estimator.fit(X[training_validation_set, :].todense(), y[training_validation_set])\n",
    "        print(\"  Making the estimator\", flush=True)\n",
    "        estimator = GPy.models.GPRegression(X[training_validation_set, :].todense(), np.vstack(y[training_validation_set]), kernel)\n",
    "        estimator.optimize_restarts(num_restarts = 5, messages=False)\n",
    "        \n",
    "        y_pred, y_sigma_pred = estimator.predict(np.asarray(X[test_set, :].todense()))\n",
    "        #y_pred, y_sigma_pred = estimator.predict(X[test_set, :].todense())        \n",
    "        #y_pred, y_sigma_pred = estimator.predict(X[test_set, :].todense(), return_std=True)\n",
    "        y_pred_binned = bin_width*(y_pred//bin_width)\n",
    "        y_true_binned = bin_width*(y[test_set]//bin_width)\n",
    "        results[data_source_filename][feature_set] = {'accuracy': accuracy_score(y_true_binned, y_pred_binned),\n",
    "                                                      'y_pred': y_pred_binned, 'y_true': y_true_binned,\n",
    "                                                      'y__pred_regr': y_pred, 'y_true_regr': y[test_set],\n",
    "                                                      'y_sigma_regr': y_sigma_pred}\n",
    "\n",
    "    print()\n",
    "    print(\" Results\")\n",
    "    for feature_set in results[data_source_filename].keys():\n",
    "        print(\"  %s, %.1f%%\" % (feature_set, 100*results[data_source_filename][feature_set]['accuracy']))\n",
    "        print(\"  unique predicted classes: %s\" % np.unique(results[data_source_filename][feature_set]['y_pred']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading SDHK_Latin.npz... done (1.5s)\n",
      " Saving SDHK_Latin.npz... done (6.5s)\n"
     ]
    }
   ],
   "source": [
    "for data_source_filename in results.keys():\n",
    "    dataset = load_dataset(data_source_filename)\n",
    "    print(\" Saving %s... \" % data_source_filename.split(\"/\")[-1], end=\"\")\n",
    "    t = time.time()\n",
    "    if 'gaussianprocess' in dataset.keys():\n",
    "        dataset.pop('gaussianprocess')\n",
    "    np.savez_compressed(data_source_filename, **dataset, gaussianprocess=results[data_source_filename])\n",
    "    print(\"done (%.1fs)\" % (time.time()-t), flush=True)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Dating - NB & SVM.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
